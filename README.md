# ğŸ§  OpenWebUI_Qdrant_Docker

[![Docker](https://img.shields.io/badge/Powered%20By-Docker-blue?logo=docker)](https://www.docker.com/)
[![Qdrant](https://img.shields.io/badge/Vector%20DB-Qdrant-purple?logo=data)](https://qdrant.tech/)
[![Ollama](https://img.shields.io/badge/LLMs-Ollama-brightgreen?logo=openai)](https://ollama.com)
[![Made by Stan](https://img.shields.io/badge/Created%20by-Stan-blueviolet)](#)

> A complete local AI assistant stack with RAG support using **OpenWebUI + Qdrant + Ollama** â€“ easily run LLMs like `llama3` locally and query your own documents.

---

## ğŸ“¦ Stack

| Component     | Purpose                         |
|---------------|---------------------------------|
| ğŸ—¨ï¸ OpenWebUI  | Frontend chat interface         |
| ğŸ“š Qdrant     | Vector database for embeddings  |
| ğŸ§  Ollama     | Local LLM runner (e.g. llama3)  |
| ğŸ³ Docker     | Containerized everything        |

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/spenkov101/OpenWebUI_Qdrant_Docker.git
cd OpenWebUI_Qdrant_Docker/my-work-env



![image](https://github.com/user-attachments/assets/c1d39ee4-180e-42a5-af7e-612b3b0aa937)

